{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb3ca82",
   "metadata": {},
   "source": [
    "# Vader Lexicon + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e3eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\aminm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aminm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aminm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VaderSentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "import optuna\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff765e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Datasets\n",
    "train_balanced = pd.read_csv(\"C:/Users/aminm/OneDrive/Desktop/Uni MA Master/2. Semester/Web Mining/Web Mining Project/data/balanced/train_balanced.csv\")\n",
    "val_balanced = pd.read_csv(\"C:/Users/aminm/OneDrive/Desktop/Uni MA Master/2. Semester/Web Mining/Web Mining Project/data/balanced/val_balanced.csv\")\n",
    "\n",
    "train_stratified = pd.read_csv(\"C:/Users/aminm/OneDrive/Desktop/Uni MA Master/2. Semester/Web Mining/Web Mining Project/data/stratified/train_stratified.csv\")\n",
    "val_stratified = pd.read_csv(\"C:/Users/aminm/OneDrive/Desktop/Uni MA Master/2. Semester/Web Mining/Web Mining Project/data/stratified/val_stratified.csv\")\n",
    "\n",
    "test = pd.read_csv(\"C:/Users/aminm/OneDrive/Desktop/Uni MA Master/2. Semester/Web Mining/Web Mining Project/data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726c66c",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "911d6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = str(text)\n",
    "    punctiations = string.punctuation\n",
    "    return text.translate(str.maketrans('', '', punctiations))\n",
    "\n",
    "def remove_spec_char(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcff3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase words\n",
    "train_balanced[\"full_review\"] = train_balanced[\"full_review\"].str.lower()\n",
    "val_balanced[\"full_review\"] = val_balanced[\"full_review\"].str.lower()\n",
    "train_stratified[\"full_review\"] = train_stratified[\"full_review\"].str.lower()\n",
    "val_stratified[\"full_review\"] = val_stratified[\"full_review\"].str.lower()\n",
    "test[\"full_review\"] = test[\"full_review\"].str.lower()\n",
    "# remove punctuation,...\n",
    "train_balanced[\"full_review\"] = train_balanced[\"full_review\"].apply(remove_punctuation)\n",
    "val_balanced[\"full_review\"] = val_balanced[\"full_review\"].apply(remove_punctuation)\n",
    "train_stratified[\"full_review\"] = train_stratified[\"full_review\"].apply(remove_punctuation)\n",
    "val_stratified[\"full_review\"] = val_stratified[\"full_review\"].apply(remove_punctuation)\n",
    "test[\"full_review\"] = test[\"full_review\"].apply(remove_punctuation)\n",
    "# ...special characters,...\n",
    "train_balanced[\"full_review\"] = train_balanced[\"full_review\"].apply(remove_spec_char)\n",
    "val_balanced[\"full_review\"] = val_balanced[\"full_review\"].apply(remove_spec_char)\n",
    "train_stratified[\"full_review\"] = train_stratified[\"full_review\"].apply(remove_spec_char)\n",
    "val_stratified[\"full_review\"] = val_stratified[\"full_review\"].apply(remove_spec_char)\n",
    "test[\"full_review\"] = test[\"full_review\"].apply(remove_spec_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbb8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from review text using word frequency\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "X_train_b = vectorizer.fit_transform(train_balanced['full_review'])\n",
    "X_valid_b = vectorizer.transform(val_balanced['full_review'])\n",
    "\n",
    "X_train_s = vectorizer.fit_transform(train_stratified['full_review'])\n",
    "X_valid_s = vectorizer.transform(val_stratified['full_review'])\n",
    "\n",
    "X_test = vectorizer.transform(test['full_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db32a93",
   "metadata": {},
   "source": [
    "## Apply VADER\n",
    "Valence Aware Dictionary for Sentiment Reasoning (VADER) is sensitive to both polarity (positive/negative) and intensity (strength) of emotion in texts. It takes a string and returns a dictionary of scores for each of these categories:\n",
    "   - negative\n",
    "   - neutral\n",
    "   - positive\n",
    "   - compund (computed by normalizing the scores above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d8d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from review text using sentiment lexicons\n",
    "sia = VaderSentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_scores_train_b = train_balanced['full_review'].apply(lambda x: sia.polarity_scores(x))\n",
    "X_train_lexicon_b = pd.DataFrame(list(sentiment_scores_train_b))\n",
    "sentiment_scores_valid_b = val_balanced['full_review'].apply(lambda x: sia.polarity_scores(x))\n",
    "X_valid_lexicon_b = pd.DataFrame(list(sentiment_scores_valid_b))\n",
    "\n",
    "sentiment_scores_train_s = train_stratified['full_review'].apply(lambda x: sia.polarity_scores(x))\n",
    "X_train_lexicon_s = pd.DataFrame(list(sentiment_scores_train_s))\n",
    "sentiment_scores_valid_s = val_stratified['full_review'].apply(lambda x: sia.polarity_scores(x))\n",
    "X_valid_lexicon_s = pd.DataFrame(list(sentiment_scores_valid_s))\n",
    "\n",
    "sentiment_scores_test = test['full_review'].apply(lambda x: sia.polarity_scores(x))\n",
    "X_test_lexicon = pd.DataFrame(list(sentiment_scores_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884256be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.155</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.9468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.155  0.724  0.121   -0.2565\n",
       "1  0.151  0.849  0.000   -0.6908\n",
       "2  0.000  1.000  0.000    0.0000\n",
       "3  0.000  0.663  0.337    0.9600\n",
       "4  0.000  0.674  0.326    0.9468"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lexicon_b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c0c5f",
   "metadata": {},
   "source": [
    "## Add negations\n",
    "The mark_negation class from the nltk-package highlights text parts which are negated in order to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95d0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle negation in a given sentence\n",
    "def handle_negation(sentence):\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    negated_words = mark_negation(tokenized_sentence)\n",
    "    return ' '.join(negated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d3ca501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply negation handling to training, validation, and test sets\n",
    "train_balanced['full_review'] = train_balanced['full_review'].apply(handle_negation)\n",
    "val_balanced['ReviewText'] = val_balanced['full_review'].apply(handle_negation)\n",
    "\n",
    "train_stratified['full_review'] = train_stratified['full_review'].apply(handle_negation)\n",
    "val_stratified['ReviewText'] = val_stratified['full_review'].apply(handle_negation)\n",
    "\n",
    "test['full_review'] = test['full_review'].apply(handle_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed67c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_review</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>only for terminator enthusiasts i gave this ga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worked for a few months and the kids tore it w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>didnt work_NEG but_NEG received_NEG a_NEG full...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really good graphics for a game from 2005 real...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was great until the band on top snapped in hal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>did not work_NEG im_NEG pissed_NEG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>its cool i love thebr game</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my first expirience with tb these headphones a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this is nhl 14 with a different coverea youve ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>putting these on caused the analog stick to lo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         full_review  star_rating\n",
       "0  only for terminator enthusiasts i gave this ga...            2\n",
       "1  worked for a few months and the kids tore it w...            2\n",
       "2  didnt work_NEG but_NEG received_NEG a_NEG full...            2\n",
       "3  really good graphics for a game from 2005 real...            4\n",
       "4  was great until the band on top snapped in hal...            2\n",
       "5                 did not work_NEG im_NEG pissed_NEG            1\n",
       "6                         its cool i love thebr game            5\n",
       "7  my first expirience with tb these headphones a...            2\n",
       "8  this is nhl 14 with a different coverea youve ...            1\n",
       "9  putting these on caused the analog stick to lo...            2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_balanced.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0a2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from negation-handled review text using word frequency\n",
    "X_train_negation_b = vectorizer.transform(train_balanced['full_review'])\n",
    "X_valid_negation_b = vectorizer.transform(val_balanced['full_review'])\n",
    "\n",
    "X_train_negation_s = vectorizer.transform(train_stratified['full_review'])\n",
    "X_valid_negation_s = vectorizer.transform(val_stratified['full_review'])\n",
    "\n",
    "X_test_negation = vectorizer.transform(test['full_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f806ee5",
   "metadata": {},
   "source": [
    "## Combining features and creating target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48878ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features using sparse matrices\n",
    "X_train_combined_b = hstack([X_train_lexicon_b, X_train_negation_b])\n",
    "X_valid_combined_b = hstack([X_valid_lexicon_b, X_valid_negation_b])\n",
    "\n",
    "X_train_combined_s = hstack([X_train_lexicon_s, X_train_negation_s])\n",
    "X_valid_combined_s = hstack([X_valid_lexicon_s, X_valid_negation_s])\n",
    "\n",
    "X_test_combined = hstack([X_test_lexicon, X_test_negation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c0ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variables\n",
    "y_train_b = train_balanced['star_rating']\n",
    "y_valid_b = val_balanced['star_rating']\n",
    "\n",
    "y_train_s = train_stratified['star_rating']\n",
    "y_valid_s = val_stratified['star_rating']\n",
    "\n",
    "y_test = test['star_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2600ae",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c364926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-26 10:47:01,860]\u001b[0m A new study created in memory with name: no-name-4a0355c3-8c05-40df-b2ec-2e1dcfb56a7d\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:47:01,862]\u001b[0m A new study created in memory with name: no-name-a507b9a5-7455-46b7-b722-a9eab854b635\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:47:06,203]\u001b[0m Trial 0 finished with value: 0.6965555555555556 and parameters: {'n_estimators': 331, 'max_depth': 6, 'max_features': 2448}. Best is trial 0 with value: 0.6965555555555556.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:47:20,696]\u001b[0m Trial 1 finished with value: 0.7055555555555556 and parameters: {'n_estimators': 296, 'max_depth': 17, 'max_features': 4523}. Best is trial 1 with value: 0.7055555555555556.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:47:34,587]\u001b[0m Trial 2 finished with value: 0.7082222222222222 and parameters: {'n_estimators': 202, 'max_depth': 26, 'max_features': 4360}. Best is trial 2 with value: 0.7082222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:47:41,367]\u001b[0m Trial 3 finished with value: 0.7031111111111111 and parameters: {'n_estimators': 235, 'max_depth': 11, 'max_features': 3067}. Best is trial 2 with value: 0.7082222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:47:59,006]\u001b[0m Trial 4 finished with value: 0.7073333333333334 and parameters: {'n_estimators': 278, 'max_depth': 25, 'max_features': 4293}. Best is trial 2 with value: 0.7082222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:48:16,735]\u001b[0m Trial 5 finished with value: 0.7028888888888889 and parameters: {'n_estimators': 483, 'max_depth': 11, 'max_features': 4931}. Best is trial 2 with value: 0.7082222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:48:46,064]\u001b[0m Trial 6 finished with value: 0.7058888888888889 and parameters: {'n_estimators': 496, 'max_depth': 23, 'max_features': 3614}. Best is trial 2 with value: 0.7082222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:48:49,992]\u001b[0m Trial 7 finished with value: 0.6994444444444444 and parameters: {'n_estimators': 222, 'max_depth': 18, 'max_features': 825}. Best is trial 2 with value: 0.7082222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:48:52,327]\u001b[0m Trial 8 finished with value: 0.7086666666666667 and parameters: {'n_estimators': 65, 'max_depth': 28, 'max_features': 1027}. Best is trial 8 with value: 0.7086666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:48:58,418]\u001b[0m Trial 9 finished with value: 0.697 and parameters: {'n_estimators': 213, 'max_depth': 8, 'max_features': 3590}. Best is trial 8 with value: 0.7086666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:48:59,395]\u001b[0m Trial 10 finished with value: 0.6731111111111111 and parameters: {'n_estimators': 53, 'max_depth': 29, 'max_features': 244}. Best is trial 8 with value: 0.7086666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:49:02,386]\u001b[0m Trial 11 finished with value: 0.7078888888888889 and parameters: {'n_estimators': 57, 'max_depth': 29, 'max_features': 1749}. Best is trial 8 with value: 0.7086666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:49:07,532]\u001b[0m Trial 12 finished with value: 0.7074444444444444 and parameters: {'n_estimators': 135, 'max_depth': 24, 'max_features': 1626}. Best is trial 8 with value: 0.7086666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:49:11,649]\u001b[0m Trial 13 finished with value: 0.7066666666666667 and parameters: {'n_estimators': 148, 'max_depth': 20, 'max_features': 1338}. Best is trial 8 with value: 0.7086666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:49:33,494]\u001b[0m Trial 14 finished with value: 0.7100000000000001 and parameters: {'n_estimators': 361, 'max_depth': 30, 'max_features': 2460}. Best is trial 14 with value: 0.7100000000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:49:55,916]\u001b[0m Trial 15 finished with value: 0.709 and parameters: {'n_estimators': 387, 'max_depth': 30, 'max_features': 2350}. Best is trial 14 with value: 0.7100000000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:50:19,070]\u001b[0m Trial 16 finished with value: 0.709 and parameters: {'n_estimators': 398, 'max_depth': 30, 'max_features': 2350}. Best is trial 14 with value: 0.7100000000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:50:39,580]\u001b[0m Trial 17 finished with value: 0.708 and parameters: {'n_estimators': 393, 'max_depth': 22, 'max_features': 2808}. Best is trial 14 with value: 0.7100000000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:50:50,863]\u001b[0m Trial 18 finished with value: 0.7045555555555556 and parameters: {'n_estimators': 384, 'max_depth': 14, 'max_features': 2237}. Best is trial 14 with value: 0.7100000000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:51:18,069]\u001b[0m Trial 19 finished with value: 0.7081111111111112 and parameters: {'n_estimators': 436, 'max_depth': 26, 'max_features': 3147}. Best is trial 14 with value: 0.7100000000000001.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:51:35,464]\u001b[0m Trial 0 finished with value: 0.4527777777777778 and parameters: {'n_estimators': 449, 'max_depth': 15, 'max_features': 3716}. Best is trial 0 with value: 0.4527777777777778.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:51:57,704]\u001b[0m Trial 1 finished with value: 0.45611111111111113 and parameters: {'n_estimators': 306, 'max_depth': 26, 'max_features': 3715}. Best is trial 1 with value: 0.45611111111111113.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:51:58,243]\u001b[0m Trial 2 finished with value: 0.439 and parameters: {'n_estimators': 154, 'max_depth': 5, 'max_features': 266}. Best is trial 1 with value: 0.45611111111111113.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:00,244]\u001b[0m Trial 3 finished with value: 0.45355555555555555 and parameters: {'n_estimators': 253, 'max_depth': 16, 'max_features': 323}. Best is trial 1 with value: 0.45611111111111113.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:06,801]\u001b[0m Trial 4 finished with value: 0.4493333333333333 and parameters: {'n_estimators': 209, 'max_depth': 12, 'max_features': 4215}. Best is trial 1 with value: 0.45611111111111113.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:09,597]\u001b[0m Trial 5 finished with value: 0.44233333333333336 and parameters: {'n_estimators': 87, 'max_depth': 12, 'max_features': 2235}. Best is trial 1 with value: 0.45611111111111113.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:36,361]\u001b[0m Trial 6 finished with value: 0.4547777777777778 and parameters: {'n_estimators': 331, 'max_depth': 30, 'max_features': 3233}. Best is trial 1 with value: 0.45611111111111113.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:38,604]\u001b[0m Trial 7 finished with value: 0.46066666666666667 and parameters: {'n_estimators': 377, 'max_depth': 17, 'max_features': 90}. Best is trial 7 with value: 0.46066666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:42,827]\u001b[0m Trial 8 finished with value: 0.4653333333333333 and parameters: {'n_estimators': 254, 'max_depth': 26, 'max_features': 358}. Best is trial 8 with value: 0.4653333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:56,613]\u001b[0m Trial 9 finished with value: 0.45422222222222225 and parameters: {'n_estimators': 470, 'max_depth': 14, 'max_features': 2460}. Best is trial 8 with value: 0.4653333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:52:59,180]\u001b[0m Trial 10 finished with value: 0.4513333333333333 and parameters: {'n_estimators': 53, 'max_depth': 23, 'max_features': 1636}. Best is trial 8 with value: 0.4653333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:53:11,223]\u001b[0m Trial 11 finished with value: 0.46255555555555555 and parameters: {'n_estimators': 382, 'max_depth': 22, 'max_features': 1129}. Best is trial 8 with value: 0.4653333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:53:22,567]\u001b[0m Trial 12 finished with value: 0.4567777777777778 and parameters: {'n_estimators': 382, 'max_depth': 22, 'max_features': 1065}. Best is trial 8 with value: 0.4653333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:53:30,863]\u001b[0m Trial 13 finished with value: 0.46011111111111114 and parameters: {'n_estimators': 241, 'max_depth': 22, 'max_features': 1198}. Best is trial 8 with value: 0.4653333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:53:46,323]\u001b[0m Trial 14 finished with value: 0.4662222222222222 and parameters: {'n_estimators': 388, 'max_depth': 30, 'max_features': 942}. Best is trial 14 with value: 0.4662222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:54:01,766]\u001b[0m Trial 15 finished with value: 0.4483333333333333 and parameters: {'n_estimators': 172, 'max_depth': 30, 'max_features': 4954}. Best is trial 14 with value: 0.4662222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:54:10,195]\u001b[0m Trial 16 finished with value: 0.4613333333333333 and parameters: {'n_estimators': 308, 'max_depth': 27, 'max_features': 690}. Best is trial 14 with value: 0.4662222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:54:31,108]\u001b[0m Trial 17 finished with value: 0.46011111111111114 and parameters: {'n_estimators': 431, 'max_depth': 26, 'max_features': 1671}. Best is trial 14 with value: 0.4662222222222222.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-26 10:54:50,750]\u001b[0m Trial 18 finished with value: 0.46444444444444444 and parameters: {'n_estimators': 354, 'max_depth': 28, 'max_features': 1905}. Best is trial 14 with value: 0.4662222222222222.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 10:54:55,250]\u001b[0m Trial 19 finished with value: 0.45866666666666667 and parameters: {'n_estimators': 278, 'max_depth': 19, 'max_features': 692}. Best is trial 14 with value: 0.4662222222222222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score for X_train_combined_s: 0.7100000000000001\n",
      "Best Hyperparameters for X_train_combined_s: {'n_estimators': 361, 'max_depth': 30, 'max_features': 2460}\n",
      "Best F1 Score for X_train_combined_b: 0.4662222222222222\n",
      "Best Hyperparameters for X_train_combined_b: {'n_estimators': 388, 'max_depth': 30, 'max_features': 942}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial, X_train, y_train, X_valid, y_valid):\n",
    "    # Define the hyperparameters to tune\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'max_features': trial.suggest_int('max_features', 1, 5000)\n",
    "    }\n",
    "\n",
    "    # Train a Random Forest model with the current hyperparameters\n",
    "    model = RandomForestClassifier(**params, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(y_valid, y_pred_valid, average='micro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "study_s = optuna.create_study(direction='maximize')  # Maximize the F1 score for dataset X_train_combined_s\n",
    "study_b = optuna.create_study(direction='maximize')  # Maximize the F1 score for dataset X_train_combined_b\n",
    "\n",
    "# Optimize the objective function for dataset X_train_combined_s\n",
    "study_s.optimize(lambda trial: objective(trial, X_train_combined_s, y_train_s, X_valid_combined_s, y_valid_s),\n",
    "                 n_trials=20)\n",
    "\n",
    "# Optimize the objective function for dataset X_train_combined_b\n",
    "study_b.optimize(lambda trial: objective(trial, X_train_combined_b, y_train_b, X_valid_combined_b, y_valid_b),\n",
    "                 n_trials=20)\n",
    "\n",
    "# Print the best F1 score and the best set of hyperparameters for each dataset\n",
    "print('Best F1 Score for X_train_combined_s:', study_s.best_value)\n",
    "print('Best Hyperparameters for X_train_combined_s:', study_s.best_params)\n",
    "print('Best F1 Score for X_train_combined_b:', study_b.best_value)\n",
    "print('Best Hyperparameters for X_train_combined_b:', study_b.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ea16a",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "### Stratified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "516d518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.7154873914541072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.65      0.58     15453\n",
      "           2       0.17      0.00      0.01      6680\n",
      "           3       0.42      0.07      0.12     11173\n",
      "           4       0.37      0.03      0.06     22197\n",
      "           5       0.75      0.97      0.85    109980\n",
      "\n",
      "    accuracy                           0.72    165483\n",
      "   macro avg       0.45      0.35      0.32    165483\n",
      "weighted avg       0.63      0.72      0.63    165483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_s = RandomForestClassifier(n_estimators = 361, max_depth = 30, max_features = 2460)\n",
    "model_s.fit(X_train_combined_s, y_train_s)\n",
    "\n",
    "y_pred_s = model_s.predict(X_test_combined)\n",
    "\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_s, average=\"micro\"))\n",
    "\n",
    "print(classification_report(y_test, y_pred_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee1cf0",
   "metadata": {},
   "source": [
    "### Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ceed9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.5799085102397226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.72      0.54     15453\n",
      "           2       0.16      0.27      0.20      6680\n",
      "           3       0.22      0.27      0.24     11173\n",
      "           4       0.25      0.39      0.30     22197\n",
      "           5       0.89      0.65      0.75    109980\n",
      "\n",
      "    accuracy                           0.58    165483\n",
      "   macro avg       0.39      0.46      0.41    165483\n",
      "weighted avg       0.69      0.58      0.62    165483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_b = RandomForestClassifier(n_estimators = 388, max_depth = 30, max_features = 942)\n",
    "model_b.fit(X_train_combined_b, y_train_b)\n",
    "\n",
    "y_pred_b = model_b.predict(X_test_combined)\n",
    "\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_b, average=\"micro\"))\n",
    "\n",
    "print(classification_report(y_test, y_pred_b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
