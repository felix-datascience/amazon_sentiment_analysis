{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/felix/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/felix/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 381.15 ms\n",
      "Type conversion took: 204.73 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 301.18 ms\n",
      "Type conversion took: 210.35 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 288.56 ms\n",
      "Type conversion took: 203.96 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 267.04 ms\n",
      "Type conversion took: 195.00 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 254.21 ms\n",
      "Type conversion took: 192.30 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 285.15 ms\n",
      "Type conversion took: 206.40 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 318.69 ms\n",
      "Type conversion took: 228.17 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 299.66 ms\n",
      "Type conversion took: 215.99 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 338.64 ms\n",
      "Type conversion took: 249.24 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 476.82 ms\n",
      "Type conversion took: 303.05 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 382.67 ms\n",
      "Type conversion took: 281.78 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 491.54 ms\n",
      "Type conversion took: 346.88 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 433.90 ms\n",
      "Type conversion took: 290.16 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 359.15 ms\n",
      "Type conversion took: 253.60 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 366.12 ms\n",
      "Type conversion took: 258.73 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 324.10 ms\n",
      "Type conversion took: 228.47 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 555.55 ms\n",
      "Type conversion took: 343.28 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 564.88 ms\n",
      "Type conversion took: 368.86 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 614.17 ms\n",
      "Type conversion took: 388.39 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 604.87 ms\n",
      "Type conversion took: 383.02 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 635.55 ms\n",
      "Type conversion took: 388.79 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 591.83 ms\n",
      "Type conversion took: 388.49 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 589.85 ms\n",
      "Type conversion took: 387.21 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 697.36 ms\n",
      "Type conversion took: 422.66 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 737.08 ms\n",
      "Type conversion took: 464.99 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 678.40 ms\n",
      "Type conversion took: 394.91 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 534.44 ms\n",
      "Type conversion took: 361.43 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Tokenization took: 70.60 ms\n",
      "Type conversion took: 39.18 ms\n",
      "Parser memory cleanup took: 0.01 ms\n"
     ]
    }
   ],
   "source": [
    "# read data into pandas dataframe\n",
    "path = \"data/video_games.tsv.gz\"\n",
    "video_games = pd.read_csv(path, sep=\"\\t\", verbose=True, parse_dates=[14], on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an amazing joystick. I especially love that yo...</td>\n",
       "      <td>Used this for Elite Dangerous on my mac, an am...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Definitely a silent mouse... Not a single clic...</td>\n",
       "      <td>Loved it,  I didn't even realise it was a gami...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One Star</td>\n",
       "      <td>poor quality work and not as it is advertised.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good, but could be bettee</td>\n",
       "      <td>nice, but tend to slip away from stick in inte...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great but flawed.</td>\n",
       "      <td>Great amiibo, great for collecting. Quality ma...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     review_headline  \\\n",
       "0  an amazing joystick. I especially love that yo...   \n",
       "1  Definitely a silent mouse... Not a single clic...   \n",
       "2                                           One Star   \n",
       "3                          good, but could be bettee   \n",
       "4                                  Great but flawed.   \n",
       "\n",
       "                                         review_body  star_rating  \n",
       "0  Used this for Elite Dangerous on my mac, an am...            5  \n",
       "1  Loved it,  I didn't even realise it was a gami...            5  \n",
       "2     poor quality work and not as it is advertised.            1  \n",
       "3  nice, but tend to slip away from stick in inte...            3  \n",
       "4  Great amiibo, great for collecting. Quality ma...            4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_games = video_games[[\"review_headline\", \"review_body\", \"star_rating\"]]\n",
    "\n",
    "video_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an amazing joystick. I especially love that yo...</td>\n",
       "      <td>Used this for Elite Dangerous on my mac, an am...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Definitely a silent mouse... Not a single clic...</td>\n",
       "      <td>Loved it,  I didn't even realise it was a gami...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One Star</td>\n",
       "      <td>poor quality work and not as it is advertised.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good, but could be bettee</td>\n",
       "      <td>nice, but tend to slip away from stick in inte...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great but flawed.</td>\n",
       "      <td>Great amiibo, great for collecting. Quality ma...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     review_headline  \\\n",
       "0  an amazing joystick. I especially love that yo...   \n",
       "1  Definitely a silent mouse... Not a single clic...   \n",
       "2                                           One Star   \n",
       "3                          good, but could be bettee   \n",
       "4                                  Great but flawed.   \n",
       "\n",
       "                                         review_body  star_rating  sentiment  \n",
       "0  Used this for Elite Dangerous on my mac, an am...            5          1  \n",
       "1  Loved it,  I didn't even realise it was a gami...            5          1  \n",
       "2     poor quality work and not as it is advertised.            1          0  \n",
       "3  nice, but tend to slip away from stick in inte...            3          0  \n",
       "4  Great amiibo, great for collecting. Quality ma...            4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary sentiment values\n",
    "# star ratings of 1-3 -> 0 (negative)\n",
    "# star ratings of 4-5 -> 1 (positive)\n",
    "\n",
    "video_games[\"sentiment\"] = video_games[\"star_rating\"].map({1: 0, 2: 0, 3: 0, 4: 1, 5: 1})\n",
    "\n",
    "video_games.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used for preprocessing\n",
    "\n",
    "def tokenize_words(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    return tokenized_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = str(text)\n",
    "    punctiations = string.punctuation\n",
    "    return text.translate(str.maketrans('', '', punctiations))\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(plain_text):\n",
    "    return ' '.join([word for word in plain_text.split() if word not in STOPWORDS])\n",
    "\n",
    "def remove_spec_char(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#wordnet_map = {'N':wordnet.NOUN, 'V':wordnet.VERB, 'J':wordnet.ADJ, 'R':wordnet.ADV}\n",
    "#def lemmatize_word(plain_text):\n",
    "#    # Finind pos tags\n",
    "#    pos_text = pos_tag(plain_text.split())\n",
    "#    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return ' '.join([ps.stem(word) for word in text.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amaz joystick especi love twist</td>\n",
       "      <td>Used this for Elite Dangerous on my mac, an am...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>definit silent mous singl click heard</td>\n",
       "      <td>Loved it,  I didn't even realise it was a gami...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one star</td>\n",
       "      <td>poor quality work and not as it is advertised.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good could bette</td>\n",
       "      <td>nice, but tend to slip away from stick in inte...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great flaw</td>\n",
       "      <td>Great amiibo, great for collecting. Quality ma...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         review_headline  \\\n",
       "0        amaz joystick especi love twist   \n",
       "1  definit silent mous singl click heard   \n",
       "2                               one star   \n",
       "3                       good could bette   \n",
       "4                             great flaw   \n",
       "\n",
       "                                         review_body  star_rating  sentiment  \n",
       "0  Used this for Elite Dangerous on my mac, an am...            5          1  \n",
       "1  Loved it,  I didn't even realise it was a gami...            5          1  \n",
       "2     poor quality work and not as it is advertised.            1          0  \n",
       "3  nice, but tend to slip away from stick in inte...            3          0  \n",
       "4  Great amiibo, great for collecting. Quality ma...            4          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase words\n",
    "video_games[\"review_headline\"] = video_games[\"review_headline\"].str.lower()\n",
    "# remove punctuation,...\n",
    "video_games[\"review_headline\"] = video_games[\"review_headline\"].apply(remove_punctuation)\n",
    "# ...special characters,...\n",
    "video_games[\"review_headline\"] = video_games[\"review_headline\"].apply(remove_spec_char)\n",
    "# ...and stopwords\n",
    "video_games[\"review_headline\"] = video_games[\"review_headline\"].apply(remove_stopwords)\n",
    "# apply stemming\n",
    "video_games[\"review_headline\"] = video_games[\"review_headline\"].apply(stem_words)\n",
    "\n",
    "video_games.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872330\n",
      "373857\n",
      "534081\n"
     ]
    }
   ],
   "source": [
    "# create train, validation and test splits\n",
    "train_df, test_df = train_test_split(video_games, test_size=0.3)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3)\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bindary sentiment values\n",
    "y_train_bin = train_df[\"sentiment\"].values\n",
    "y_val_bin = val_df[\"sentiment\"].values\n",
    "y_test_bin = test_df[\"sentiment\"].values\n",
    "\n",
    "# star ratings\n",
    "y_train_stars = train_df[\"star_rating\"].values\n",
    "y_val_stars = val_df[\"star_rating\"].values\n",
    "y_test_stars = test_df[\"star_rating\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation - binary word occurence vectors (only review headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of terms: 54062\n"
     ]
    }
   ],
   "source": [
    "# binary word occurence vectors\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df[\"review_headline\"])\n",
    "X_val = vectorizer.transform(val_df[\"review_headline\"])\n",
    "X_test = vectorizer.transform(test_df[\"review_headline\"])\n",
    "\n",
    "print(f\"number of terms: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid Baseline Model\n",
    "\n",
    "**Predicting the most common class...**\n",
    "\n",
    "### Predicting Sentiment (star ratings 1, 2 and 3: negative; star ratings 4 and 5: positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "acc: 0.7531736842708608\n",
      "pre: 0.7531736842708608\n",
      "rec: 1.0\n",
      "validation set:\n",
      "acc: 0.7529456449926041\n",
      "pre: 0.7529456449926041\n",
      "rec: 1.0\n",
      "test set:\n",
      "acc: 0.7529138838490791\n",
      "pre: 0.7529138838490791\n",
      "rec: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"train set:\")\n",
    "print(f\"acc: {accuracy_score(y_train_bin, [1 for _ in range(len(y_train_bin))])}\")\n",
    "print(f\"pre: {precision_score(y_train_bin, [1 for _ in range(len(y_train_bin))])}\")\n",
    "print(f\"rec: {recall_score(y_train_bin, [1 for _ in range(len(y_train_bin))])}\")\n",
    "print(\"validation set:\")\n",
    "print(f\"acc: {accuracy_score(y_val_bin, [1 for _ in range(len(y_val_bin))])}\")\n",
    "print(f\"pre: {precision_score(y_val_bin, [1 for _ in range(len(y_val_bin))])}\")\n",
    "print(f\"rec: {recall_score(y_val_bin, [1 for _ in range(len(y_val_bin))])}\")\n",
    "print(\"test set:\")\n",
    "print(f\"acc: {accuracy_score(y_test_bin, [1 for _ in range(len(y_test_bin))])}\")\n",
    "print(f\"pre: {precision_score(y_test_bin, [1 for _ in range(len(y_test_bin))])}\")\n",
    "print(f\"rec: {recall_score(y_test_bin, [1 for _ in range(len(y_test_bin))])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Star Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "rmse: 1.6503482381086434\n",
      "validation set:\n",
      "rmse: 1.6501169726009062\n",
      "test set:\n",
      "rmse: 1.650271459302246\n"
     ]
    }
   ],
   "source": [
    "print(\"train set:\")\n",
    "print(f\"rmse: {math.sqrt(mean_squared_error(y_train_stars, [5 for _ in range(len(y_train_stars))]))}\")\n",
    "print(\"validation set:\")\n",
    "print(f\"rmse: {math.sqrt(mean_squared_error(y_val_stars, [5 for _ in range(len(y_val_stars))]))}\")\n",
    "print(\"test set:\")\n",
    "print(f\"rmse: {math.sqrt(mean_squared_error(y_test_stars, [5 for _ in range(len(y_test_stars))]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model\n",
    "\n",
    "### Predicting Sentiment (star ratings 1, 2 and 3: negative; star ratings 4 and 5: positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "acc: 0.8474854699482994\n",
      "pre: 0.874927908524325\n",
      "rec: 0.9305237619783993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.59      0.66    215314\n",
      "           1       0.87      0.93      0.90    657016\n",
      "\n",
      "    accuracy                           0.85    872330\n",
      "   macro avg       0.81      0.76      0.78    872330\n",
      "weighted avg       0.84      0.85      0.84    872330\n",
      "\n",
      "validation set:\n",
      "acc: 0.838539869522304\n",
      "pre: 0.8670528674578803\n",
      "rec: 0.9278279465992171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.57      0.63     92363\n",
      "           1       0.87      0.93      0.90    281494\n",
      "\n",
      "    accuracy                           0.84    373857\n",
      "   macro avg       0.79      0.75      0.77    373857\n",
      "weighted avg       0.83      0.84      0.83    373857\n",
      "\n",
      "test set:\n",
      "acc: 0.8379290781735355\n",
      "pre: 0.8666243760979282\n",
      "rec: 0.9274837920306777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.57      0.63    131964\n",
      "           1       0.87      0.93      0.90    402117\n",
      "\n",
      "    accuracy                           0.84    534081\n",
      "   macro avg       0.79      0.75      0.76    534081\n",
      "weighted avg       0.83      0.84      0.83    534081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = BernoulliNB()\n",
    "\n",
    "naive_bayes.fit(X_train, y_train_bin)\n",
    "\n",
    "y_pred_bin_train = naive_bayes.predict(X_train)\n",
    "y_pred_bin_val = naive_bayes.predict(X_val)\n",
    "y_pred_bin_test = naive_bayes.predict(X_test)\n",
    "\n",
    "print(\"train set:\")\n",
    "print(f\"acc: {accuracy_score(y_train_bin, y_pred_bin_train)}\")\n",
    "print(f\"pre: {precision_score(y_train_bin, y_pred_bin_train)}\")\n",
    "print(f\"rec: {recall_score(y_train_bin, y_pred_bin_train)}\")\n",
    "print(classification_report(y_train_bin, y_pred_bin_train))\n",
    "print(\"validation set:\")\n",
    "print(f\"acc: {accuracy_score(y_val_bin, y_pred_bin_val)}\")\n",
    "print(f\"pre: {precision_score(y_val_bin, y_pred_bin_val)}\")\n",
    "print(f\"rec: {recall_score(y_val_bin, y_pred_bin_val)}\")\n",
    "print(classification_report(y_val_bin, y_pred_bin_val))\n",
    "print(\"test set:\")\n",
    "print(f\"acc: {accuracy_score(y_test_bin, y_pred_bin_test)}\")\n",
    "print(f\"pre: {precision_score(y_test_bin, y_pred_bin_test)}\")\n",
    "print(f\"rec: {recall_score(y_test_bin, y_pred_bin_test)}\")\n",
    "print(classification_report(y_test_bin, y_pred_bin_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Star Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "rmse: 1.2458439155874828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.57      0.57     94008\n",
      "           2       0.40      0.08      0.13     46149\n",
      "           3       0.56      0.30      0.39     75157\n",
      "           4       0.52      0.32      0.40    155824\n",
      "           5       0.73      0.92      0.81    501192\n",
      "\n",
      "    accuracy                           0.68    872330\n",
      "   macro avg       0.55      0.44      0.46    872330\n",
      "weighted avg       0.64      0.68      0.64    872330\n",
      "\n",
      "validation set:\n",
      "rmse: 1.2732793121766508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.54      0.54     40184\n",
      "           2       0.25      0.04      0.07     19936\n",
      "           3       0.50      0.26      0.34     32243\n",
      "           4       0.47      0.28      0.35     66630\n",
      "           5       0.72      0.91      0.80    214864\n",
      "\n",
      "    accuracy                           0.66    373857\n",
      "   macro avg       0.50      0.41      0.42    373857\n",
      "weighted avg       0.61      0.66      0.62    373857\n",
      "\n",
      "test set:\n",
      "rmse: 1.2761666347233762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.54      0.54     57441\n",
      "           2       0.25      0.04      0.07     28472\n",
      "           3       0.49      0.26      0.34     46051\n",
      "           4       0.46      0.28      0.35     95006\n",
      "           5       0.72      0.91      0.80    307111\n",
      "\n",
      "    accuracy                           0.66    534081\n",
      "   macro avg       0.49      0.41      0.42    534081\n",
      "weighted avg       0.61      0.66      0.61    534081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_stars = BernoulliNB()\n",
    "\n",
    "naive_bayes_stars.fit(X_train, y_train_stars)\n",
    "\n",
    "y_pred_stars_train= naive_bayes_stars.predict(X_train)\n",
    "y_pred_stars_val = naive_bayes_stars.predict(X_val)\n",
    "y_pred_stars_test = naive_bayes_stars.predict(X_test)\n",
    "\n",
    "print(\"train set:\")\n",
    "print(f\"rmse: {math.sqrt(mean_squared_error(y_train_stars, y_pred_stars_train))}\")\n",
    "print(classification_report(y_train_stars, y_pred_stars_train))\n",
    "print(\"validation set:\")\n",
    "print(f\"rmse: {math.sqrt(mean_squared_error(y_val_stars, y_pred_stars_val))}\")\n",
    "print(classification_report(y_val_stars, y_pred_stars_val))\n",
    "print(\"test set:\")\n",
    "print(f\"rmse: {math.sqrt(mean_squared_error(y_test_stars, y_pred_stars_test))}\")\n",
    "print(classification_report(y_test_stars, y_pred_stars_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
